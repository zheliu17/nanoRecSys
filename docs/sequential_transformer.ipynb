{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Sequential Transformer (Retrieval & Ranking)\n",
    "\n",
    "This notebook creates a  Sequential Recommender System using the MovieLens-20M dataset. It covers the full training pipeline:\n",
    "\n",
    "1.  **Data Processing:** Converting raw MovieLens data into user sequence arrays.\n",
    "2.  **Retriever Training:** Training a SASRec-style Transformer with RoPE, SwiGLU, and InfoNCE loss.\n",
    "3.  **Ranking:** Mining hard negatives and training a secondary MLP Ranker.\n",
    "\n",
    "### Protocol\n",
    "*   **Data Split:** Leave-One-Out (LOO).\n",
    "*   **Positive Samples:** All interactions treated as implicit feedback.\n",
    "*   **Metrics:** HR@10, NDCG@10 (no sampling, full ranking, and exclude items the user has already interacted with)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup (Colab)\n",
    "Run the following cell to install the package and dependencies if running in Google Colab.\n",
    "If running locally, ensure you have installed the package via `pip install -e .`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 620
    },
    "id": "pVBK-db6-IUU",
    "outputId": "2c4d0c58-5591-49c0-bc7f-e1f4fa1c6642"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/zheliu17/nanoRecSys.git\n",
    "%pip install -q -e ./nanoRecSys\n",
    "\n",
    "import psutil  # noqa: F401\n",
    "\n",
    "# In fact, we don't need psutil. force-reinstall to trigger colab restart\n",
    "%pip install --force-reinstall psutil=={psutil.__version__}\n",
    "print(\"Installation complete. Please restart runtime...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Preparation\n",
    "We use the MovieLens-20M dataset:\n",
    "- Chronologically sorting user interactions.\n",
    "- Splitting data: The last item of each user sequence is reserved for the `test` set. The second to last is for `val` set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7mrL1Mq5AXx-",
    "outputId": "dbd2f807-676a-418f-ad0c-f77fce1c22e6"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import nanoRecSys.data.build_dataset\n",
    "import nanoRecSys.data.splits\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "nanoRecSys.data.build_dataset.process_data()\n",
    "nanoRecSys.data.splits.create_user_time_split()\n",
    "nanoRecSys.data.build_dataset.prebuild_sequential_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Training the Retriever (Sequential Transformer)\n",
    "\n",
    "We train a **Transformer-based tower** using **InfoNCE loss**.\n",
    "\n",
    "Training takes ~10 hours on A100 GPU for 300 epochs (100 epochs is sufficient for good performance).\n",
    "\n",
    "> **Note:** You may also download the pre-trained model from [huggingface](https://huggingface.co/zheliu97/nanoRecSys) and place it in the `artifacts` directory to skip training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "6f631e1e0a424930a755332d14cd4f8e",
      "d2cfd0ff5fb9428d9d004e3906a5c230"
     ]
    },
    "id": "O_a9X5g4AYID",
    "outputId": "f75fbb9f-3a09-4b59-fc6c-8e30ab928b21"
   },
   "outputs": [],
   "source": [
    "import nanoRecSys.train\n",
    "\n",
    "\n",
    "class Args:\n",
    "    mode = \"retriever\"\n",
    "    user_tower_type = \"transformer\"\n",
    "\n",
    "    epochs = 300\n",
    "    batch_size = 128\n",
    "    lr = 1e-3\n",
    "    num_workers = 4\n",
    "    warmup_steps = 2000\n",
    "    ckpt_path = \"last\"\n",
    "\n",
    "\n",
    "nanoRecSys.train.main(Args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate Embeddings for Indexing\n",
    "\n",
    "Once trained, we generate static embeddings for all items. For users, we can pre-compute embeddings for the static validation/test set.\n",
    "Make sure `item_tower.pth` and `user_tower.pth` are in the `artifacts` directory (`nanoRecSys.config.settings.artifacts_dir`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nanoRecSys.indexing.build_embeddings\n",
    "\n",
    "nanoRecSys.indexing.build_embeddings.build_item_embeddings(batch_size=128)\n",
    "nanoRecSys.indexing.build_embeddings.build_user_embeddings(batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Retriever evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nanoRecSys.eval.offline_eval import OfflineEvaluator\n",
    "\n",
    "evaluator = OfflineEvaluator(1024)\n",
    "results = evaluator.eval_retrieval()\n",
    "\n",
    "df = evaluator.formatted_results(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Negative Mining & Ranker Training\n",
    "\n",
    "The retriever is good at finding relevant items from millions, but a heavier Ranker can re-order them for better precision.\n",
    "\n",
    "Takes ~10 minutes in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "71afd9e7e22140ee934ad09994509b7a",
      "c44f696ba9a245c58cf4cba6b94ab387"
     ]
    },
    "id": "SPMUOBJlQKpp",
    "outputId": "76b5046a-2a79-48ea-b0c8-70e740cf98f4"
   },
   "outputs": [],
   "source": [
    "import nanoRecSys.training.mine_negatives_sasrec\n",
    "import nanoRecSys.train\n",
    "\n",
    "# Generate hard negatives per interaction. Full set takes 10GB+ disk space.\n",
    "# Here we only keep 0.2 ratio of them.\n",
    "nanoRecSys.training.mine_negatives_sasrec.run_pipeline(\n",
    "    batch_size=128, top_k=100, skip_top=10, sampling_ratio=0.2\n",
    ")\n",
    "\n",
    "\n",
    "class Args:\n",
    "    mode = \"ranker\"\n",
    "    user_tower_type = \"transformer\"\n",
    "    epochs = 5\n",
    "    batch_size = 2048\n",
    "    id_dropout = 0.5\n",
    "    random_neg_ratio = 0.01\n",
    "\n",
    "    lr = 1e-3\n",
    "    item_lr = 0\n",
    "    num_workers = 2\n",
    "    warmup_steps = 500\n",
    "    check_val_every_n_epoch = 1\n",
    "\n",
    "\n",
    "nanoRecSys.train.main(Args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ranker Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 261
    },
    "id": "UDwpw1icSosH",
    "outputId": "eac0a950-a911-4829-e3d9-7222704e668d"
   },
   "outputs": [],
   "source": [
    "from nanoRecSys.eval.offline_eval import OfflineEvaluator\n",
    "\n",
    "evaluator = OfflineEvaluator(1024)\n",
    "results = evaluator.eval_popularity()\n",
    "\n",
    "df = evaluator.formatted_results(results)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "id": "4T7n5oKOSk8Z",
    "outputId": "ed4b06d9-97b5-4be6-e677-5b9cb538281b"
   },
   "outputs": [],
   "source": [
    "results = evaluator.eval_ranker()\n",
    "# results = evaluator.eval_ranker_new_items()\n",
    "\n",
    "df = evaluator.formatted_results(results)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
